{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e662cf22-28a0-4533-b2b8-f8e1715fcada",
   "metadata": {},
   "source": [
    "# COUNT(1) vs COUNT(*) vs COUNT(COL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526666cf-8ac8-409f-89dd-512a6be00011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/31 16:44:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b0a315f65ea6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Count(1) vs Count(*)</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f127dd59c10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Spark Session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Count(1) vs Count(*)\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69801eee-aec0-47f9-96f4-6c2ccf671a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----------+--------------------+-------+----------+\n",
      "|      transacted_at|    trx_id|retailer_id|         description| amount|   city_id|\n",
      "+-------------------+----------+-----------+--------------------+-------+----------+\n",
      "|2017-12-21 22:00:00|1982756348| 1953761884|Home Depot       ...|  11.51|1194163531|\n",
      "|2017-12-06 12:00:00|2091468861| 2001148981|              Costco|  35.26| 957346984|\n",
      "|2017-12-18 18:00:00|1043640082|  606497335|unkn      Bishkek...| 101.37|2056066328|\n",
      "|2017-12-03 21:00:00|1980557911| 1076023740|unkn    ppd id: 5...|    5.8|1223420625|\n",
      "|2017-11-27 12:00:00|1292471097|  643354906|unkn    ccd id: 6...|  90.79|  77397141|\n",
      "|2017-12-26 19:00:00| 848534109|  400404203|CVS  arc id: 1070264|   4.33|2074005445|\n",
      "|2017-11-28 14:00:00|1256298623|  847200066|Wal-Mart     arc ...| 476.57| 720701459|\n",
      "|2017-10-03 21:00:00|1525949111|  103953879|Rite Aid       Ha...|  30.13| 576817662|\n",
      "|2017-12-29 23:00:00| 436713004| 1273066548|7-Eleven  ccd id:...|  11.68|1690909999|\n",
      "|2017-08-31 19:00:00|2127859528|  511877722|unkn     ccd id: ...|  45.24|1345953582|\n",
      "|2017-12-08 21:00:00|1628235995|  771821475|Domino's Pizza   ...|  22.11| 414653088|\n",
      "|2017-12-09 12:00:00|1614112617|  562903918|McDonald's    ppd...|  35.22|1602735059|\n",
      "|2017-11-29 18:00:00| 656307318|  997626433|Sears     ccd id:...|  345.7| 781290085|\n",
      "|2017-11-27 12:00:00|1288467700|  562903918|unkn  arc id: 106...| 216.13|1717498102|\n",
      "|2017-11-27 23:00:00|1336921350| 1076023740|unkn        Suva ...| 336.85| 687701170|\n",
      "|2017-12-18 23:00:00| 632923914| 1318092070|unkn  ccd id: 117...|  60.61| 485114748|\n",
      "|2017-12-30 12:00:00|2008095984|  400404203|CVS     ccd id: 3...|  21.17|2074005445|\n",
      "|2017-12-03 18:00:00| 471627664|  354400420|Subway        Pod...|   24.0|1759612211|\n",
      "|2017-01-21 19:00:00|1460097173|  683159064|unkn        Linco...| 2969.2|1717498102|\n",
      "|2017-12-28 13:00:00|1356785799|  511877722|Best Buy   arc id...|1798.35|1620965190|\n",
      "+-------------------+----------+-----------+--------------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets read the dataframe to check the data\n",
    "df = spark.read.format(\"parquet\").load(\"hdfs://namenode:9000/input/data/sales.parquet\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "510f9b06-4f6f-49cd-a69b-561b1d449f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10464096-da89-498a-be92-45808a495330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['trx_id], ['trx_id, count(1) AS count(1)#71L]\n",
      "+- Relation [transacted_at#0,trx_id#1L,retailer_id#2L,description#3,amount#4,city_id#5L] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "trx_id: bigint, count(1): bigint\n",
      "Aggregate [trx_id#1L], [trx_id#1L, count(1) AS count(1)#71L]\n",
      "+- Relation [transacted_at#0,trx_id#1L,retailer_id#2L,description#3,amount#4,city_id#5L] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [trx_id#1L], [trx_id#1L, count(1) AS count(1)#71L]\n",
      "+- Project [trx_id#1L]\n",
      "   +- Relation [transacted_at#0,trx_id#1L,retailer_id#2L,description#3,amount#4,city_id#5L] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[trx_id#1L], functions=[count(1)], output=[trx_id#1L, count(1)#71L])\n",
      "+- Exchange hashpartitioning(trx_id#1L, 200), ENSURE_REQUIREMENTS, [plan_id=95]\n",
      "   +- *(1) HashAggregate(keys=[trx_id#1L], functions=[partial_count(1)], output=[trx_id#1L, count#75L])\n",
      "      +- *(1) ColumnarToRow\n",
      "         +- FileScan parquet [trx_id#1L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://namenode:9000/input/data/sales.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<trx_id:bigint>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark.sparkContext.setJobDescription(\"save count(1)\")\n",
    "df.groupBy(\"trx_id\").agg(F.count(F.lit(1))).explain(True)\n",
    "df.groupBy(\"trx_id\").agg(F.count(F.lit(1))).write.format(\"noop\").mode(\n",
    "    \"overwrite\"\n",
    ").save()\n",
    "spark.sparkContext.setJobDescription(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290e8312-4fe3-4f90-8725-005b25b88a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['trx_id], ['trx_id, count('city_id) AS count(city_id)#97]\n",
      "+- Relation [transacted_at#0,trx_id#1L,retailer_id#2L,description#3,amount#4,city_id#5L] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "trx_id: bigint, count(city_id): bigint\n",
      "Aggregate [trx_id#1L], [trx_id#1L, count(city_id#5L) AS count(city_id)#97L]\n",
      "+- Relation [transacted_at#0,trx_id#1L,retailer_id#2L,description#3,amount#4,city_id#5L] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [trx_id#1L], [trx_id#1L, count(city_id#5L) AS count(city_id)#97L]\n",
      "+- Project [trx_id#1L, city_id#5L]\n",
      "   +- Relation [transacted_at#0,trx_id#1L,retailer_id#2L,description#3,amount#4,city_id#5L] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[trx_id#1L], functions=[count(city_id#5L)], output=[trx_id#1L, count(city_id)#97L])\n",
      "+- Exchange hashpartitioning(trx_id#1L, 200), ENSURE_REQUIREMENTS, [plan_id=166]\n",
      "   +- *(1) HashAggregate(keys=[trx_id#1L], functions=[partial_count(city_id#5L)], output=[trx_id#1L, count#101L])\n",
      "      +- *(1) ColumnarToRow\n",
      "         +- FileScan parquet [trx_id#1L,city_id#5L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://namenode:9000/input/data/sales.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<trx_id:bigint,city_id:bigint>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get count(col_name) performance\n",
    "spark.sparkContext.setJobDescription(\"save count(city_id)\")\n",
    "df.groupBy(\"trx_id\").agg(F.count(\"city_id\")).explain(True)\n",
    "df.groupBy(\"trx_id\").agg(F.count(\"city_id\")).write.format(\"noop\").mode(\n",
    "    \"overwrite\"\n",
    ").save()\n",
    "spark.sparkContext.setJobDescription(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "863ac0ef-7829-42b6-90a4-2f83d1b48dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['trx_id], ['trx_id, count(1) AS count(1)#125L]\n",
      "+- Relation [transacted_at#0,trx_id#1L,retailer_id#2L,description#3,amount#4,city_id#5L] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "trx_id: bigint, count(1): bigint\n",
      "Aggregate [trx_id#1L], [trx_id#1L, count(1) AS count(1)#125L]\n",
      "+- Relation [transacted_at#0,trx_id#1L,retailer_id#2L,description#3,amount#4,city_id#5L] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [trx_id#1L], [trx_id#1L, count(1) AS count(1)#125L]\n",
      "+- Project [trx_id#1L]\n",
      "   +- Relation [transacted_at#0,trx_id#1L,retailer_id#2L,description#3,amount#4,city_id#5L] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[trx_id#1L], functions=[count(1)], output=[trx_id#1L, count(1)#125L])\n",
      "+- Exchange hashpartitioning(trx_id#1L, 200), ENSURE_REQUIREMENTS, [plan_id=237]\n",
      "   +- *(1) HashAggregate(keys=[trx_id#1L], functions=[partial_count(1)], output=[trx_id#1L, count#129L])\n",
      "      +- *(1) ColumnarToRow\n",
      "         +- FileScan parquet [trx_id#1L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://namenode:9000/input/data/sales.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<trx_id:bigint>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get count(*) performance\n",
    "spark.sparkContext.setJobDescription(\"save count(*)\")\n",
    "df.groupBy(\"trx_id\").agg(F.count(\"*\")).explain(True)\n",
    "df.groupBy(\"trx_id\").agg(F.count(\"*\")).write.format(\"noop\").mode(\"overwrite\").save()\n",
    "spark.sparkContext.setJobDescription(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "495463a8-086c-4dd1-b5ae-6de3fd67b8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['trx_id], ['trx_id, count(1) AS count#151L]\n",
      "+- Relation [transacted_at#0,trx_id#1L,retailer_id#2L,description#3,amount#4,city_id#5L] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "trx_id: bigint, count: bigint\n",
      "Aggregate [trx_id#1L], [trx_id#1L, count(1) AS count#151L]\n",
      "+- Relation [transacted_at#0,trx_id#1L,retailer_id#2L,description#3,amount#4,city_id#5L] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [trx_id#1L], [trx_id#1L, count(1) AS count#151L]\n",
      "+- Project [trx_id#1L]\n",
      "   +- Relation [transacted_at#0,trx_id#1L,retailer_id#2L,description#3,amount#4,city_id#5L] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[trx_id#1L], functions=[count(1)], output=[trx_id#1L, count#151L])\n",
      "+- Exchange hashpartitioning(trx_id#1L, 200), ENSURE_REQUIREMENTS, [plan_id=308]\n",
      "   +- *(1) HashAggregate(keys=[trx_id#1L], functions=[partial_count(1)], output=[trx_id#1L, count#155L])\n",
      "      +- *(1) ColumnarToRow\n",
      "         +- FileScan parquet [trx_id#1L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://namenode:9000/input/data/sales.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<trx_id:bigint>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get count() performance\n",
    "spark.sparkContext.setJobDescription(\"save count()\")\n",
    "df.groupBy(\"trx_id\").count().explain(True)\n",
    "df.groupBy(\"trx_id\").count().write.format(\"noop\").mode(\"overwrite\").save()\n",
    "spark.sparkContext.setJobDescription(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2d4b0f-1fca-4ca0-88c4-b58113cd662a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['trx_id], ['trx_id, count(1) AS count(1)#177L]\n",
      "+- Filter (month(cast(transacted_at#0 as date)) = 11)\n",
      "   +- Relation [transacted_at#0,trx_id#1L,retailer_id#2L,description#3,amount#4,city_id#5L] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "trx_id: bigint, count(1): bigint\n",
      "Aggregate [trx_id#1L], [trx_id#1L, count(1) AS count(1)#177L]\n",
      "+- Filter (month(cast(transacted_at#0 as date)) = 11)\n",
      "   +- Relation [transacted_at#0,trx_id#1L,retailer_id#2L,description#3,amount#4,city_id#5L] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [trx_id#1L], [trx_id#1L, count(1) AS count(1)#177L]\n",
      "+- Project [trx_id#1L]\n",
      "   +- Filter (isnotnull(transacted_at#0) AND (month(cast(transacted_at#0 as date)) = 11))\n",
      "      +- Relation [transacted_at#0,trx_id#1L,retailer_id#2L,description#3,amount#4,city_id#5L] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[trx_id#1L], functions=[count(1)], output=[trx_id#1L, count(1)#177L])\n",
      "+- Exchange hashpartitioning(trx_id#1L, 200), ENSURE_REQUIREMENTS, [plan_id=387]\n",
      "   +- *(1) HashAggregate(keys=[trx_id#1L], functions=[partial_count(1)], output=[trx_id#1L, count#181L])\n",
      "      +- *(1) Project [trx_id#1L]\n",
      "         +- *(1) Filter (isnotnull(transacted_at#0) AND (month(cast(transacted_at#0 as date)) = 11))\n",
      "            +- *(1) ColumnarToRow\n",
      "               +- FileScan parquet [transacted_at#0,trx_id#1L] Batched: true, DataFilters: [isnotnull(transacted_at#0), (month(cast(transacted_at#0 as date)) = 11)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://namenode:9000/input/data/sales.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(transacted_at)], ReadSchema: struct<transacted_at:string,trx_id:bigint>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get filter + count(*) performance\n",
    "spark.sparkContext.setJobDescription(\"save filter-count(*)\")\n",
    "df.filter(F.month(F.col(\"transacted_at\")) == 11).groupBy(\"trx_id\").agg(\n",
    "    F.count(\"*\")\n",
    ").explain(True)\n",
    "df.filter(F.month(F.col(\"transacted_at\")) == 11).groupBy(\"trx_id\").agg(\n",
    "    F.count(\"*\")\n",
    ").write.format(\"noop\").mode(\"overwrite\").save()\n",
    "spark.sparkContext.setJobDescription(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2336c58b-32f3-4ada-bed7-db834417f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f16ee2-ff90-4dce-82fa-c20ab6de6f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
