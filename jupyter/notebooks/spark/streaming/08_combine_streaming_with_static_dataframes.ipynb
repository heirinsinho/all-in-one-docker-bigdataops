{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c65853-e887-4fe4-801c-db178a8e701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Streaming with Static DF\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1035dcb-153d-4608-8298-b5b1a00d62bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially unset (= 200). Try both values\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842b95c3-4fd6-4ed6-95ba-4e5b0e5a2d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Rates a spark monotonically increasing id dataframe with a rate of 1/second\n",
    "df_streaming = spark.readStream.format(\"rate\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f9377-ec9a-4a5e-a1eb-0a546698a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the rate output before streaming to kafka\n",
    "# df_streaming \\\n",
    "# .writeStream \\\n",
    "# .format(\"console\") \\\n",
    "# .option(\"truncate\", False) \\\n",
    "# .outputMode(\"update\") \\\n",
    "# .trigger(processingTime=\"2 seconds\") \\\n",
    "# .start() \\\n",
    "# .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4c7ab0-11b3-4621-860e-b78e3f40b5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the static employee dataframe\n",
    "df_static = spark.read.csv(\n",
    "    \"hdfs://namenode:9000/input/data/employee_records.csv\",\n",
    "    inferSchema=True,\n",
    "    header=True,\n",
    ")\n",
    "df_static = df_static.filter(\n",
    "    F.col(\"salary\") < 100000\n",
    ")  # Apply this filter to reduce the size\n",
    "df_static.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81476dd-28cd-4981-84b8-48802efc5246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are going to join both dataframes using the id rate column as a dept_id\n",
    "df_streaming = df_streaming.withColumn(\"department_id\", F.col(\"value\") % 11)\n",
    "df = df_streaming.join(df_static, on=\"department_id\")\n",
    "\n",
    "# TEST WITH DIFFERENT PROCESSING_TIMES AND WITH DIFFERENT SQL.SHUFFLE.PARTITIONS\n",
    "\n",
    "df.writeStream.format(\"console\").option(\"truncate\", False).outputMode(\"update\").trigger(\n",
    "    processingTime=\"8 seconds\"\n",
    ").start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df10693-a469-46f9-8333-a23b24b52041",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
