{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92f7e43a-550e-4b11-8a42-57a2a022d950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/13 17:43:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Spark Introduction\")\n",
    "    .config(\"spark.executor.instances\", \"2\")\n",
    "    .config(\"spark.executor.cores\", \"1\")\n",
    "    .config(\"spark.executor.memory\", \"1g\") \n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d85b5ec-1e9f-4f03-80ca-029cf50a6518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://7ed9d7ed5e4d:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Introduction</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0cd225b090>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0656d31-ba16-49f3-8f7f-f0c038d4d886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emp Data & Schema\n",
    "\n",
    "emp_data = [\n",
    "    [\"001\",\"101\",\"John Doe\",\"30\",\"Male\",\"50000\",\"2015-01-01\"],\n",
    "    [\"002\",\"101\",\"Jane Smith\",\"25\",\"Female\",\"45000\",\"2016-02-15\"],\n",
    "    [\"003\",\"102\",\"Bob Brown\",\"35\",\"Male\",\"55000\",\"2014-05-01\"],\n",
    "    [\"004\",\"102\",\"Alice Lee\",\"28\",\"Female\",\"48000\",\"2017-09-30\"],\n",
    "    [\"005\",\"103\",\"Jack Chan\",\"40\",\"Male\",\"60000\",\"2013-04-01\"],\n",
    "    [\"006\",\"103\",\"Jill Wong\",\"32\",\"Female\",\"52000\",\"2018-07-01\"],\n",
    "    [\"007\",\"101\",\"James Johnson\",\"42\",\"Male\",\"70000\",\"2012-03-15\"],\n",
    "    [\"008\",\"102\",\"Kate Kim\",\"29\",\"Female\",\"51000\",\"2019-10-01\"],\n",
    "    [\"009\",\"103\",\"Tom Tan\",\"33\",\"Male\",\"58000\",\"2016-06-01\"],\n",
    "    [\"010\",\"104\",\"Lisa Lee\",\"27\",\"Female\",\"47000\",\"2018-08-01\"],\n",
    "    [\"011\",\"104\",\"David Park\",\"38\",\"Male\",\"65000\",\"2015-11-01\"],\n",
    "    [\"012\",\"105\",\"Susan Chen\",\"31\",\"Female\",\"54000\",\"2017-02-15\"],\n",
    "    [\"013\",\"106\",\"Brian Kim\",\"45\",\"Male\",\"75000\",\"2011-07-01\"],\n",
    "    [\"014\",\"107\",\"Emily Lee\",\"26\",\"Female\",\"46000\",\"2019-01-01\"],\n",
    "    [\"015\",\"106\",\"Michael Lee\",\"37\",\"Male\",\"63000\",\"2014-09-30\"],\n",
    "    [\"016\",\"107\",\"Kelly Zhang\",\"30\",\"Female\",\"49000\",\"2018-04-01\"],\n",
    "    [\"017\",\"105\",\"George Wang\",\"34\",\"Male\",\"57000\",\"2016-03-15\"],\n",
    "    [\"018\",\"104\",\"Nancy Liu\",\"29\",\"Female\",\"50000\",\"2017-06-01\"],\n",
    "    [\"019\",\"103\",\"Steven Chen\",\"36\",\"Male\",\"62000\",\"2015-08-01\"],\n",
    "    [\"020\",\"102\",\"Grace Kim\",\"32\",\"Female\",\"53000\",\"2018-11-01\"]\n",
    "]\n",
    "\n",
    "emp_schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1cd7a52-3086-43a9-8e64-dd87eee86c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create emp DataFrame\n",
    "\n",
    "emp = spark.createDataFrame(data=emp_data, schema=emp_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb5c8a76-ab66-42b9-b885-475b7259c4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(employee_id='001', department_id='101', name='John Doe', age='30', gender='Male', salary='50000', hire_date='2015-01-01'),\n",
       " Row(employee_id='002', department_id='101', name='Jane Smith', age='25', gender='Female', salary='45000', hire_date='2016-02-15'),\n",
       " Row(employee_id='003', department_id='102', name='Bob Brown', age='35', gender='Male', salary='55000', hire_date='2014-05-01'),\n",
       " Row(employee_id='004', department_id='102', name='Alice Lee', age='28', gender='Female', salary='48000', hire_date='2017-09-30'),\n",
       " Row(employee_id='005', department_id='103', name='Jack Chan', age='40', gender='Male', salary='60000', hire_date='2013-04-01'),\n",
       " Row(employee_id='006', department_id='103', name='Jill Wong', age='32', gender='Female', salary='52000', hire_date='2018-07-01'),\n",
       " Row(employee_id='007', department_id='101', name='James Johnson', age='42', gender='Male', salary='70000', hire_date='2012-03-15'),\n",
       " Row(employee_id='008', department_id='102', name='Kate Kim', age='29', gender='Female', salary='51000', hire_date='2019-10-01'),\n",
       " Row(employee_id='009', department_id='103', name='Tom Tan', age='33', gender='Male', salary='58000', hire_date='2016-06-01'),\n",
       " Row(employee_id='010', department_id='104', name='Lisa Lee', age='27', gender='Female', salary='47000', hire_date='2018-08-01'),\n",
       " Row(employee_id='011', department_id='104', name='David Park', age='38', gender='Male', salary='65000', hire_date='2015-11-01'),\n",
       " Row(employee_id='012', department_id='105', name='Susan Chen', age='31', gender='Female', salary='54000', hire_date='2017-02-15'),\n",
       " Row(employee_id='013', department_id='106', name='Brian Kim', age='45', gender='Male', salary='75000', hire_date='2011-07-01'),\n",
       " Row(employee_id='014', department_id='107', name='Emily Lee', age='26', gender='Female', salary='46000', hire_date='2019-01-01'),\n",
       " Row(employee_id='015', department_id='106', name='Michael Lee', age='37', gender='Male', salary='63000', hire_date='2014-09-30'),\n",
       " Row(employee_id='016', department_id='107', name='Kelly Zhang', age='30', gender='Female', salary='49000', hire_date='2018-04-01'),\n",
       " Row(employee_id='017', department_id='105', name='George Wang', age='34', gender='Male', salary='57000', hire_date='2016-03-15'),\n",
       " Row(employee_id='018', department_id='104', name='Nancy Liu', age='29', gender='Female', salary='50000', hire_date='2017-06-01'),\n",
       " Row(employee_id='019', department_id='103', name='Steven Chen', age='36', gender='Male', salary='62000', hire_date='2015-08-01'),\n",
       " Row(employee_id='020', department_id='102', name='Grace Kim', age='32', gender='Female', salary='53000', hire_date='2018-11-01')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ea6550d-86fd-46fd-aeb1-e0ba7c8c84be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of partitions\n",
    "\n",
    "emp.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ed4a06b-34e1-443e-ba41-fa2f04f336ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+------------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|partition_id|\n",
      "+-----------+-------------+-------------+---+------+------+----------+------------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|           0|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|           0|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|           1|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|           1|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|           2|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|           2|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|           3|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|           3|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|           3|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|           3|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|           4|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|           4|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|           5|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|           5|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|           6|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|           6|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|           7|\n",
      "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|           7|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|           7|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|           7|\n",
      "+-----------+-------------+-------------+---+------+------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "emp = emp.withColumn(\"partition_id\", F.spark_partition_id())\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "058c1ba4-e2b8-4ff3-9d3e-5ed2d8169fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor unknown is processing partition 0\n",
      "Executor unknown is processing partition 1\n",
      "Executor unknown is processing partition 2\n",
      "Executor unknown is processing partition 3\n",
      "Executor unknown is processing partition 4\n",
      "Executor unknown is processing partition 5\n",
      "Executor unknown is processing partition 6\n",
      "Executor unknown is processing partition 7\n"
     ]
    }
   ],
   "source": [
    "from pyspark import TaskContext\n",
    "import os\n",
    "\n",
    "def get_executor_partition_info(index, iterator):\n",
    "    context = TaskContext.get()\n",
    "    partition_id = context.partitionId()\n",
    "    executor_id = os.getenv(\"SPARK_EXECUTOR_ID\", \"unknown\")\n",
    "    \n",
    "    # Return a string containing executor and partition information\n",
    "    return [f\"Executor {executor_id} is processing partition {partition_id}\"]\n",
    "\n",
    "# Apply the function and collect the results\n",
    "info = emp.rdd.mapPartitionsWithIndex(get_executor_partition_info).collect()\n",
    "\n",
    "# Print the collected information on the driver\n",
    "for line in info:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea3b0dfd-c9ab-49a8-9c72-1ebeb3450d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor 7d01e3fcfccf is processing partition 0\n",
      "Executor 57c92dd270c0 is processing partition 1\n",
      "Executor 7d01e3fcfccf is processing partition 2\n",
      "Executor 57c92dd270c0 is processing partition 3\n",
      "Executor 7d01e3fcfccf is processing partition 4\n",
      "Executor 57c92dd270c0 is processing partition 5\n",
      "Executor 7d01e3fcfccf is processing partition 6\n",
      "Executor 57c92dd270c0 is processing partition 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "def get_executor_host(index, iterator):\n",
    "    # Get the hostname of the machine processing this partition (i.e., the executor)\n",
    "    executor_host = socket.gethostname()\n",
    "    \n",
    "    # Get partition ID from TaskContext\n",
    "    context = TaskContext.get()\n",
    "    partition_id = context.partitionId()\n",
    "\n",
    "    return [f\"Executor {executor_host} is processing partition {partition_id}\"]\n",
    "\n",
    "# Apply the function and collect the results\n",
    "info = emp.rdd.mapPartitionsWithIndex(get_executor_host).collect()\n",
    "\n",
    "# Print the collected information on the driver\n",
    "for line in info:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a9a9ddc-0500-487f-bb7f-01f9314f9372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'(8) MapPartitionsRDD[17] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[16] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  SQLExecutionRDD[15] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[14] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[4] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[3] at map at SerDeUtil.scala:69 []\\n |  MapPartitionsRDD[2] at mapPartitions at SerDeUtil.scala:117 []\\n |  PythonRDD[1] at RDD at PythonRDD.scala:53 []\\n |  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:287 []'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp.rdd.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a983c2e6-90c3-4f80-9b42-5e281a53d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write our first Transformation (EMP salary > 50000)\n",
    "\n",
    "emp_final1 = emp.where(\"salary > 50000\")\n",
    "emp_final2 = emp.filter(\"salary > 50000\")\n",
    "emp_final3 = emp.filter(F.col(\"salary\") > 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2870aea-2bf3-411c-aefe-155f44808f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(salary#5) AND (cast(salary#5 as int) > 50000))\n",
      "+- *(1) Project [employee_id#0, department_id#1, name#2, age#3, gender#4, salary#5, hire_date#6, SPARK_PARTITION_ID() AS partition_id#43]\n",
      "   +- *(1) Scan ExistingRDD[employee_id#0,department_id#1,name#2,age#3,gender#4,salary#5,hire_date#6]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_final1.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556440e4-94d1-4881-8773-b31f8ac19b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_final2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2df2f1-49ed-4059-8b7c-2e96a0cdbcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate number of Partitions\n",
    "\n",
    "emp_final.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf1ed98-c722-437c-8ccc-872ab8d593d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data as CSV output (ACTION)\n",
    "\n",
    "emp_final.write.format(\"csv\").save(\"data/output/1/emp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b2608-7592-4056-9543-8b858bc80090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
